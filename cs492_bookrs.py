# -*- coding: utf-8 -*-
"""CS492-BookRS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GAf1CNnjbAt6U5ZP2P4cNNTj-s37X1kS
"""

#First: importing all libraries
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import csv

#uploading csv files
from google.colab import files
files.upload()

#read everything, later on execlude things..
books = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding="latin-1")
books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']


users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding="latin-1")
users.columns = ['userID', 'Location', 'Age']

ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding="latin-1")
ratings.columns = ['userID', 'ISBN', 'bookRating']

#test 1: creating a new data set containing: ISBN, UserID, BookTitle, bookRating
newDataSet = pd.merge(ratings, books, on='ISBN')
UselessColumns = ['yearOfPublication', 'publisher', 'bookAuthor', 'imageUrlS', 'imageUrlM', 'imageUrlL']
newDataSet = newDataSet.drop(UselessColumns, axis=1)    #drop useless columns

newDataSet.dropna(axis = 0, subset = ['bookTitle'])
newBookRating = (newDataSet.
     groupby(by = ['bookTitle'])['bookRating'].
     count().
     reset_index().
     rename(columns = {'bookRating': 'totalRatingCount'})
     [['bookTitle', 'totalRatingCount']]        #count total counts of ratings for each book
    )
print("\n", newDataSet.head())      #test output, gives top 5 of the table

BooksRatingwithCount = newDataSet.merge(newBookRating, left_on = 'bookTitle', right_on = 'bookTitle', how = 'left')
print("\n", BooksRatingwithCount.head())

#test output of popularity, the threshold was set to 50, so we'll leave it as it is
popularity_threshold = 50
popularBook = BooksRatingwithCount.query('totalRatingCount >= @popularity_threshold')
print("\n", popularBook.head())

#testing ability to get a graph out the previous output:
booksPopG = nx.Graph()
for booksPop in popularBook['bookTitle']:
       if booksPopG.number_of_nodes() < 5:      #top 5
           booksPopG.add_node(booksPop)

#booksPopG.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5)]) #later test links...
print("\n")
plt.figure(figsize=(5, 5))
nx.draw(booksPopG, with_labels=True)
plt.show()
booksPopG.number_of_nodes()

#plotting a graph to test successful upload
bookgraph=nx.Graph()
for t in books['bookTitle']:
       if bookgraph.number_of_nodes()<2000:#out of11123
           bookgraph.add_node(t)
plt.figure(figsize=(20,20))
nx.draw_random(bookgraph, with_labels=True)
bookgraph.number_of_nodes()

#The book with ISBN “0971880107” received the most rating counts.
rating_count = pd.DataFrame(ratings.groupby('ISBN')['bookRating'].count())
rating_count.sort_values('bookRating', ascending=False).head()

#Find out what book it is, and what books are in the top 5.
most_rated_books = pd.DataFrame(['0971880107', '0316666343', '0060928336', '0440214041', '0385504209'], index=np.arange(5), columns = ['ISBN'])
most_rated_books_summary = pd.merge(most_rated_books, books, on='ISBN')
most_rated_books_summary

#storing counts, then filtering, this will ensure statistical significance
counts1 = ratings['userID'].value_counts()
ratings = ratings[ratings['userID'].isin(counts1[counts1 >= 200].index)] #users with less than 200 ratings execluded
counts = ratings['bookRating'].value_counts()
ratings = ratings[ratings['bookRating'].isin(counts[counts >= 100].index)] #books with less than 100 ratings

#test 2:
#until now, there are no links!
newGraph = nx.Graph()
for aBook in newDataSet['ISBN']:
  if newGraph.number_of_nodes() < 1000:      #example only showing a subset of the dataset
           newGraph.add_node(aBook)

plt.figure(figsize=(20, 20))
nx.draw(newGraph, with_labels=True)
plt.show()

#test 3: adding links, what is the most logical way to link ?
#link between two users if they read the same book!
#in order for our code to work faster, we eliminated the number of points to 50

linkingGraph = nx.Graph()
i = 0
for aUser in newDataSet['userID']:
  if(linkingGraph.number_of_nodes() < 50):
    linkingGraph.add_node(i, userID=aUser, ISBN=newDataSet['ISBN'][i])
    i = i + 1

NumNodes = linkingGraph.number_of_nodes()

for k in range(NumNodes-2):
  for j in range(NumNodes-1):
    if j == 49: break;
    if linkingGraph.nodes[k]['ISBN'] == linkingGraph.nodes[j]['ISBN'] and j != k:
      linkingGraph.add_edge(k, j)

plt.figure(figsize=(20,20))
nx.draw(linkingGraph, with_labels=True)
plt.show()

print(linkingGraph.nodes.data())

#filtering part: filter to only Canada and US users
combined = popularBook.merge(users, left_on = 'userID', right_on = 'userID', how = 'left')

us_canada_user_rating = combined[combined['Location'].str.contains("usa|canada")]
us_canada_user_rating=us_canada_user_rating.drop('Age', axis=1)
print(us_canada_user_rating.head())

#implementing kNN on the previous part



#code is working, error because we need to upload files only

from scipy.sparse import csr_matrix
us_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle'])
us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle', columns = 'userID', values = 'bookRating').fillna(0)
us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values)

from sklearn.neighbors import NearestNeighbors

model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(us_canada_user_rating_matrix)
print(model_knn)

query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])    #chooses randomly and starts building matrix of ratings
print(query_index)
print(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1,-1))
distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)
us_canada_user_rating_pivot.index[query_index]

for i in range(0, len(distances.flatten())):
    if i == 0:
        print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index]))
    else:
        print('{0}: {1}, with distance of : {2}'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i]))